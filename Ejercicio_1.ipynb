{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1v2cUm0z5TGv-s0mywn0nXNqAozu5tl6P","authorship_tag":"ABX9TyNTgEuLJInb+XTcO4aRTHOB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RNuxx5AFCNh9","executionInfo":{"status":"ok","timestamp":1700578981959,"user_tz":-60,"elapsed":1036,"user":{"displayName":"lorenzo martinez","userId":"13221369929382490449"}},"outputId":"dd7a8c87-53d7-4ccf-f382-1c0b7c7d2e80"},"outputs":[{"output_type":"stream","name":"stdout","text":["             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n","0  570306133677760513           neutral                        1.0000   \n","1  570301130888122368          positive                        0.3486   \n","2  570301083672813571           neutral                        0.6837   \n","3  570301031407624196          negative                        1.0000   \n","4  570300817074462722          negative                        1.0000   \n","\n","  negativereason  negativereason_confidence         airline  \\\n","0            NaN                        NaN  Virgin America   \n","1            NaN                     0.0000  Virgin America   \n","2            NaN                        NaN  Virgin America   \n","3     Bad Flight                     0.7033  Virgin America   \n","4     Can't Tell                     1.0000  Virgin America   \n","\n","  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n","0                    NaN     cairdin                 NaN              0   \n","1                    NaN    jnardino                 NaN              0   \n","2                    NaN  yvonnalynn                 NaN              0   \n","3                    NaN    jnardino                 NaN              0   \n","4                    NaN    jnardino                 NaN              0   \n","\n","                                                text tweet_coord  \\\n","0                @VirginAmerica What @dhepburn said.         NaN   \n","1  @VirginAmerica plus you've added commercials t...         NaN   \n","2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n","3  @VirginAmerica it's really aggressive to blast...         NaN   \n","4  @VirginAmerica and it's a really big bad thing...         NaN   \n","\n","               tweet_created tweet_location               user_timezone  \n","0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n","1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n","2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n","3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n","4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  \n"]}],"source":["import pandas as pd\n","\n","# # Cargar el archivo CSV desde el entorno local (debes subir el archivo a Google Colab) o desde una URL\n","# # Si el archivo está en tu entorno local:\n","# from google.colab import files\n","# uploaded = files.upload()\n","\n","# Si el archivo está en una URL, puedes usar pd.read_csv('URL')\n","\n","# Leer el archivo CSV y mostrar las primeras filas\n","df = pd.read_csv('/content/drive/MyDrive/Datos/Tweets.csv')\n","\n","# Mostrar las primeras filas del DataFrame (por ejemplo, las primeras 5 filas)\n","print(df.head())\n"]},{"cell_type":"code","source":["# Suponiendo que 'df' es tu DataFrame que contiene los datos del CSV\n","\n","# Mostrar las primeras filas del DataFrame antes de eliminar columnas\n","print(\"DataFrame original:\")\n","print(df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3htaBfFbGYxT","executionInfo":{"status":"ok","timestamp":1700578984992,"user_tz":-60,"elapsed":252,"user":{"displayName":"lorenzo martinez","userId":"13221369929382490449"}},"outputId":"b39d12b5-07f4-4aa0-c5c9-f690fe9673d7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["DataFrame original:\n","             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n","0  570306133677760513           neutral                        1.0000   \n","1  570301130888122368          positive                        0.3486   \n","2  570301083672813571           neutral                        0.6837   \n","3  570301031407624196          negative                        1.0000   \n","4  570300817074462722          negative                        1.0000   \n","\n","  negativereason  negativereason_confidence         airline  \\\n","0            NaN                        NaN  Virgin America   \n","1            NaN                     0.0000  Virgin America   \n","2            NaN                        NaN  Virgin America   \n","3     Bad Flight                     0.7033  Virgin America   \n","4     Can't Tell                     1.0000  Virgin America   \n","\n","  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n","0                    NaN     cairdin                 NaN              0   \n","1                    NaN    jnardino                 NaN              0   \n","2                    NaN  yvonnalynn                 NaN              0   \n","3                    NaN    jnardino                 NaN              0   \n","4                    NaN    jnardino                 NaN              0   \n","\n","                                                text tweet_coord  \\\n","0                @VirginAmerica What @dhepburn said.         NaN   \n","1  @VirginAmerica plus you've added commercials t...         NaN   \n","2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n","3  @VirginAmerica it's really aggressive to blast...         NaN   \n","4  @VirginAmerica and it's a really big bad thing...         NaN   \n","\n","               tweet_created tweet_location               user_timezone  \n","0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n","1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n","2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n","3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n","4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  \n"]}]},{"cell_type":"code","source":["# Seleccionar solo las columnas deseadas ('columna1' y 'columna2')\n","columnas_deseadas = ['airline_sentiment', 'text']\n","df_filtrado = df[columnas_deseadas]\n","\n","# Mostrar las primeras filas del DataFrame después de conservar solo las columnas deseadas\n","print(\"\\nDataFrame con solo las columnas deseadas:\")\n","print(df_filtrado.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PKzgpYhfFkU6","executionInfo":{"status":"ok","timestamp":1700578990901,"user_tz":-60,"elapsed":342,"user":{"displayName":"lorenzo martinez","userId":"13221369929382490449"}},"outputId":"d291d68d-aaf9-4d77-c83d-8cc546583c61"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","DataFrame con solo las columnas deseadas:\n","  airline_sentiment                                               text\n","0           neutral                @VirginAmerica What @dhepburn said.\n","1          positive  @VirginAmerica plus you've added commercials t...\n","2           neutral  @VirginAmerica I didn't today... Must mean I n...\n","3          negative  @VirginAmerica it's really aggressive to blast...\n","4          negative  @VirginAmerica and it's a really big bad thing...\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","# Supongamos que los textos están en una columna llamada 'texto'\n","textos = df_filtrado['text']\n","\n","# Inicializar el CountVectorizer para Bag of Words\n","vectorizador = CountVectorizer()\n","\n","# Aplicar el vectorizador a los textos\n","vectores_bow = vectorizador.fit_transform(textos)\n","\n","# Convertir los vectores a un DataFrame para visualización\n","df_bow = pd.DataFrame(vectores_bow.toarray(), columns=vectorizador.get_feature_names_out())\n","\n","# Mostrar los primeros documentos vectorizados usando BoW\n","print(df_bow.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F-j6QsauGTBy","executionInfo":{"status":"ok","timestamp":1700578994505,"user_tz":-60,"elapsed":1353,"user":{"displayName":"lorenzo martinez","userId":"13221369929382490449"}},"outputId":"86ddd9a2-9cb3-44c8-afaa-e139a1d1d8ae"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["   00  000  000114  000419  000ft  000lbs  0011  0016  00a  00am  ...  \\\n","0   0    0       0       0      0       0     0     0    0     0  ...   \n","1   0    0       0       0      0       0     0     0    0     0  ...   \n","2   0    0       0       0      0       0     0     0    0     0  ...   \n","3   0    0       0       0      0       0     0     0    0     0  ...   \n","4   0    0       0       0      0       0     0     0    0     0  ...   \n","\n","   zrh_airport  zsdgzydnde  zsuztnaijq  ztrdwv0n4l  zukes  zurich  zv2pt6trk9  \\\n","0            0           0           0           0      0       0           0   \n","1            0           0           0           0      0       0           0   \n","2            0           0           0           0      0       0           0   \n","3            0           0           0           0      0       0           0   \n","4            0           0           0           0      0       0           0   \n","\n","   zv6cfpohl5  zvfmxnuelj  zzps5ywve2  \n","0           0           0           0  \n","1           0           0           0  \n","2           0           0           0  \n","3           0           0           0  \n","4           0           0           0  \n","\n","[5 rows x 15051 columns]\n"]}]},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')  # Descarga el tokenizador de NLTK (solo si no está descargado)\n","\n","# Preprocesamiento y tokenización del texto\n","textos_tokenizados = [word_tokenize(text.lower()) for text in textos]\n","\n","# Entrenamiento del modelo Word2Vec\n","model = Word2Vec(sentences=textos_tokenizados, vector_size=100, window=5, min_count=1, workers=4)\n","\n","# Verificación del modelo: encontrar palabras similares a una palabra específica\n","palabra_similar = 'airline'\n","palabras_similares = model.wv.most_similar(palabra_similar)\n","\n","# Mostrar palabras similares a 'airline'\n","print(f\"Palabras similares a '{palabra_similar}':\")\n","for palabra, similitud in palabras_similares:\n","    print(f\"{palabra}: {similitud}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0cI3LnVyN2M4","executionInfo":{"status":"ok","timestamp":1700575836593,"user_tz":-60,"elapsed":12501,"user":{"displayName":"lorenzo martinez","userId":"13221369929382490449"}},"outputId":"787d29fc-b389-49f5-ec91-6526bd6a0e4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Palabras similares a 'airline':\n","experience: 0.932531476020813\n","worst: 0.9188778400421143\n","ever: 0.9186617136001587\n","company: 0.9085237383842468\n","horrible: 0.8870206475257874\n","flying: 0.8856679201126099\n","nightmare: 0.8804594874382019\n","provided: 0.8802099227905273\n","worse: 0.8793622255325317\n","joke: 0.874126136302948\n"]}]},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","import pandas as pd\n","import nltk\n","\n","# Filtrar datos por sentimiento específico (por ejemplo, 'positivo')\n","sentimiento_especifico = 'positive'\n","textos = df_filtrado[df_filtrado['airline_sentiment'] == sentimiento_especifico]['text']\n","\n","# Tokenizar y preprocesar los textos\n","textos_tokenizados = [word_tokenize(text.lower()) for text in textos]\n","\n","# Entrenar un modelo Word2Vec para el sentimiento específico\n","model = Word2Vec(sentences=textos_tokenizados, vector_size=100, window=5, min_count=1, workers=4)\n","\n","# Verificar el modelo: encontrar palabras similares a una palabra específica\n","palabra_similar = 'agradable'\n","palabras_similares = model.wv.most_similar(palabra_similar)\n","\n","# Mostrar palabras similares a 'agradable' en el contexto del sentimiento específico\n","print(f\"Palabras similares a '{palabra_similar}' en el contexto de '{sentimiento_especifico}':\")\n","for palabra, similitud in palabras_similares:\n","    print(f\"{palabra}: {similitud}\")\n"],"metadata":{"id":"tnaK_RkrN2Uy","colab":{"base_uri":"https://localhost:8080/","height":772},"executionInfo":{"status":"error","timestamp":1700579481712,"user_tz":-60,"elapsed":409,"user":{"displayName":"lorenzo martinez","userId":"13221369929382490449"}},"outputId":"43bc93ae-08da-4ba9-f0f9-c81062a1e5d1"},"execution_count":11,"outputs":[{"output_type":"error","ename":"LookupError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-6a1a72a9283f>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Tokenizar y preprocesar los textos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtextos_tokenizados\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtextos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Entrenar un modelo Word2Vec para el sentimiento específico\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-6a1a72a9283f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Tokenizar y preprocesar los textos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtextos_tokenizados\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtextos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Entrenar un modelo Word2Vec para el sentimiento específico\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"aRnLt3q9bSM-"}}]}